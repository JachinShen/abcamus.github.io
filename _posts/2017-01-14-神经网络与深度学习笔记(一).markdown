---
layout:	post
title:	"神经网络与深度学习（一）"
date:	2017-01-14
categories:	神经网络
tags: 神经网络
excerpt: 针对很多问题，基于神经网络实现简单算法+好的训练数据，效果大于复杂算法。就手写数字识别而言，经过良好训练的神经网络识别率甚至超过人脑。神经网络的速度慢是在训练阶段，就和人的学习阶段一样，等到训练完成之后，在其他平台甚至移动平台上可以快速的响应输入。
---

* menu
{:toc}

这周开始考虑在基于MP实现一些特定应用的时候，其实我心中是有几个雏形的，但是涉及到技术细节的时候，还是有些缺乏，所以就想找本书能提供些参考。巧合的是，发现了这本《神经网络与深度学习》，正好也是基于python的，有用武之地。

### 概念

本章通过神经网络来识别手写数字，在这一章中涉及到以下几个重要对象：

#### 感知器，对特定输入向量输入0或者1，等效于与非门。
#### S神经元，激活函数为sigmoid函数的感知器。
![sigmod函数](https://github.com/abcamus/abcamus.github.io/raw/master/_pic/sigmod.png)

为什么选用sigmod函数？
在现阶段看来，一是由于函数是平滑的，二是由于函数很好的收敛于0和1。
#### 梯度下降法

>Mathjax语法支持测试
$$E=MC^2$$


这是一个很简单的算法，由于梯度方向是保证目标函数变小，被用来迭代寻找最优解。

### 参考代码
相关python代码在[代码仓库](https://github.com/mnielsen/neural-networks-and-deep-learning.git)
主要用到了numpy的向量运算。

### 关于权重和偏置的合理解释
为什么要解释？
如果说现在拿到一个问题，马上就套用某某神经网络去训练，然后可能运气比较好，得到了正确率很高的结果，那么我们又如何看待这样的过程呢？其实神经网络也好，其他算法也好，说到底还是要从问题出发，去理解问题，理解问题是怎么在这个算法中逐步求解的，这样得到的结果才是可信的，这样的算法才是可持续的。所以对权重和偏置的解释能够帮助我们解答哪些问题适合通过感知器这样的网络来解决，哪些不适合。
目前文中的解释是带一些猜想性质的解释，例如把隐含层的神经元输出解释成组成数字的一部分图像。
